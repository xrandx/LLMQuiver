# LLMQuiver

The auxiliary tool is used to invoke the online LLM service API, supporting local caching, prompt rendering, and configuration management.


## Supported
- Support Openai/Azure LLM service providers (or openai-compatible service providers).
- Local caching (based on sqlite).
- Prompt rendering (based on toml and jinja2).
- Configuration management (based on toml).

## To Be Done
- Multi service provider support.